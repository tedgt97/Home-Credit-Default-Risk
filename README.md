# Home Credit Default Risk

This repository implements a full **end-to-end machine learning pipeline** for the [Home Credit Default Risk](https://www.kaggle.com/competitions/home-credit-default-risk) competition.  
The goal is to predict the probability that a client will default on a loan, based on demographic, financial, and credit history data.

The project covers:
- Reproducible project structure (data, src, models, reports)
- Data ingestion, cleaning, feature engineering, merging
- Exploratory Data Analysis (EDA) and issue tracking
- Multiple ML models: Logistic Regression, Random Forest
- Ensemble logic (L1, L2 voting schemes)
- Cross-validation, evaluation (AUC, F1, precision/recall)
- Reports, metrics, and plots exported for transparency

---

## ðŸ“‚ Repository Structure

home-credit-default-risk/
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw Kaggle data (ignored in git)
â”‚ â”œâ”€â”€ processed/ # Processed train/test parquet files
â”‚
â”œâ”€â”€ models/ # Saved trained models (.joblib)
â”œâ”€â”€ reports/
â”‚ â”œâ”€â”€ metrics/ # Excel/JSON metrics reports
â”‚ â””â”€â”€ figures/ # ROC curves, feature importances
â”‚
â”œâ”€â”€ notebooks/ # Jupyter notebooks for EDA
â”œâ”€â”€ src/
â”‚ â””â”€â”€ mlport/ # Python source package
â”‚ â”œâ”€â”€ common/ # data.py, merge.py, clean.py, score.py
â”‚ â”œâ”€â”€ pipelines/ # make_dataset.py
â”‚ â”œâ”€â”€ logistic/ # train_cv.py, predict.py
â”‚ â”œâ”€â”€ random_forest/ # train_cv.py, predict.py
â”‚ â””â”€â”€ ensemble/ # eval_oof.py, predict.py
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ pyproject.toml # Project metadata
â””â”€â”€ README.md


---

## Dataset Description

From [Kaggle](https://www.kaggle.com/competitions/home-credit-default-risk/data):

- **application_train.csv / application_test.csv**: core client and loan application data (demographics, income, loan info, target label in train only).
- **bureau.csv / bureau_balance.csv**: credit history from other institutions.
- **previous_application.csv**: history of previous loan applications.
- **installments_payments.csv**: repayment history (DPD/DBD, payment behavior).
- **credit_card_balance.csv, POS_CASH_balance.csv**: monthly credit activity (not used in this pipeline, but extendable).

**Target variable**:  
`TARGET = 1` â†’ client defaulted on loan  
`TARGET = 0` â†’ client repaid

---

## Pipeline Overview

### 1. Data Ingestion
- `mlport.common.data`: utilities for loading (`.csv`, `.parquet`) and Kaggle API download.

### 2. Feature Engineering
- `mlport.common.merge`:
  - **Bureau**: aggregated overdue, debt, credit types.
  - **Previous Applications**: counts, credit/application ratios.
  - **Installments**: delay (DPD/DBD), payment percents/differences.
- Merge into `application_train` only (test kept separate).

### 3. Cleaning
- `mlport.common.clean`:
  - Coalesce `EXT_SOURCE_1/2/3` â†’ `c_EXT_SOURCE`.
  - Normalize categorical strings.
  - Drop missing critical features (`TARGET`, `c_EXT_SOURCE`).
  - Keep only selected modeling columns.

### 4. Dataset Build
- `mlport.pipelines.make_dataset`:
  - Outputs `train.parquet`, `test.parquet` in `data/processed/`.
  - Also saves issue logs (NaN filters) to Excel for transparency.

### 5. Modeling
#### Logistic Regression (`mlport.logistic.train_cv`)
- **Stratified K-fold CV** (default=5).
- Numeric screening: keep top features by correlation with `TARGET`, drop multicollinear ones.
- Preprocessing: imputation, scaling, one-hot encoding.
- Model: `LogisticRegression(penalty="l1", solver="saga")`.

#### Random Forest (`mlport.random_forest.train_cv`)
- Stratified K-fold CV.
- Handles missing via imputation, one-hot for categoricals.
- Outputs feature importances.

#### Ensemble (`mlport.ensemble.eval_oof`)
- **L1 (liberal):** predicts default if *any* model flags default.  
- **L2 (conservative):** predicts default only if *both* models flag default.  
- Evaluation of combined predictions.

### 6. Evaluation
- Metrics: AUC, Accuracy, Precision, Recall, F1.
- Thresholds set by prevalence (match positive rate).
- Reports:
  - Excel: per-fold metrics, overall metrics.
  - JSON: summary for programmatic use.
  - Figures: ROC curves, feature importance.

### 7. Prediction
- `mlport.logistic.predict`, `mlport.random_forest.predict`, `mlport.ensemble.predict`.
- Score the processed test dataset â†’ probabilities + labels.
- Outputs `.csv` with appended prediction columns.

---

## Results Snapshot

- Logistic Regression: ~AUC 0.68â€“0.70 on CV.
- Random Forest: ~AUC 0.69â€“0.71 on CV.
- Ensemble L1/L2: balances recall vs precision.

*(Exact numbers depend on seed, folds, preprocessing.)*

---

## How to Run

### 1. Setup
```bash
git clone https://github.com/tedgt97/home-credit-default-risk.git
cd home-credit-default-risk
python -m venv .venv
.venv\Scripts\activate      # Windows PowerShell
pip install -r requirements.txt
pip install -e .
```

### 2. Download data (Kaggle API)
```bash
python -m mlport.common.data kaggle --competition home-credit-default-risk --out data/raw
```

### 3. Make dataset
```bash
python -m mlport.pipelines.make_dataset
```

### 4. Train models
```bash
python -m mlport.logistic.train_cv
python -m mlport.random_forest.train_cv --class_weight_balanced
```

### 5. Ensemble evaluation
```bash
python -m mlport.ensemble.eval_oof
```

### 6. Predict on test
```bash
python -m mlport.logistic.predict
python -m mlport.random_forest.predict
python -m mlport.ensemble.predict
```

---
## Key Algorithms Used

- **Feature Aggregation**  
  Aggregations on `bureau`, `previous_application`, and `installments` tables  
  (e.g., overdue days, credit ratios, delayed payments).

- **Feature Engineering**  
  Derived features such as `APP_CREDIT_PERC`, `DPD` (days past due), and coalesced `c_EXT_SOURCE`.

- **Logistic Regression (L1 penalty)**  
  Sparse, interpretable model with feature selection through regularization.

- **Random Forest Classifier**  
  Non-linear model capturing interactions, with feature importance analysis.

- **Ensemble Voting**  
  - **L1 (liberal):** classify as default if *any* model predicts default.  
  - **L2 (conservative):** classify as default only if *both* models predict default.

---
## Future Work

- **Clustering-Based Model (M3)**  
  Add clustering methods such as KMeans and GMM with threshold rules for risk segmentation.

- **Hyperparameter Optimization**  
  Perform advanced tuning using `GridSearchCV` or `Optuna`.

- **Model Explainability**  
  Incorporate tools such as **SHAP** and **LIME** to improve interpretability.

- **Deployment**  
  Build a demo application using **FastAPI** or **Streamlit** for interactive use.

- **CI/CD Integration**  
  Add continuous integration and deployment pipelines with **GitHub Actions**.

---
## Acknowledgments

- **Kaggle**: Home Credit Default Risk dataset.  
- **Libraries**: scikit-learn, pandas, numpy, matplotlib, tqdm.  
- **Inspiration**: Risk modeling practices in credit analytics.  
